{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Brats Model 2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyCT4wzOybmC",
        "colab_type": "code",
        "outputId": "29d3ccaf-6e67-4c2c-d6ff-bd6d04735b81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 11864113949564533286\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 15351628376638560098\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNN2zjlVybmI",
        "colab_type": "code",
        "outputId": "d6705d82-624d-41b5-8fc5-1159c30a720d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from keras import backend as K\n",
        "K.tensorflow_backend._get_available_gpus()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-Ag4hH2ybmM",
        "colab_type": "code",
        "outputId": "093fb26a-7552-4b8b-ef9c-1eefe4fb39e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "from glob import glob\n",
        "from scipy import ndimage\n",
        "from keras.models import Model,load_model\n",
        "from keras.layers.advanced_activations import PReLU\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.layers import Dropout,GaussianNoise, Input,Activation\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers import  Conv2DTranspose,UpSampling2D,concatenate,add\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import  ModelCheckpoint,Callback,LearningRateScheduler\n",
        "import keras.backend as K\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suPat5w2ybmQ",
        "colab_type": "text"
      },
      "source": [
        "# Unet Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTQTqy9ZybmR",
        "colab_type": "code",
        "outputId": "47a3102b-989a-4315-a483-4ae2d8e47993",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "K.set_image_data_format(\"channels_last\")\n",
        "\n",
        " #u-net model\n",
        "class Unet_model(object):\n",
        "    \n",
        "    def __init__(self,img_shape,load_model_weights=None):\n",
        "        self.img_shape=img_shape\n",
        "        self.load_model_weights=load_model_weights\n",
        "        self.model =self.compile_unet()\n",
        "        \n",
        "    \n",
        "    def compile_unet(self):\n",
        "        \"\"\"\n",
        "        compile the U-net model\n",
        "        \"\"\"\n",
        "        i = Input(shape=self.img_shape)\n",
        "        #add gaussian noise to the first layer to combat overfitting\n",
        "        i_=GaussianNoise(0.01)(i)\n",
        "\n",
        "        i_ = Conv2D(64, 2, padding='same',data_format = 'channels_last')(i_)\n",
        "        out=self.unet(inputs=i_)\n",
        "        model = Model(input=i, output=out)\n",
        "\n",
        "        sgd = SGD(lr=0.08, momentum=0.9, decay=5e-6, nesterov=False)\n",
        "        model.compile(loss=gen_dice_loss, optimizer=sgd, metrics=[dice_whole_metric,dice_en_metric,dice_core_metric])\n",
        "       \n",
        "        #load weights if set for prediction\n",
        "        if self.load_model_weights is not None:\n",
        "            model.load_weights(self.load_model_weights)\n",
        "        print(\"Model compilation properly done\")\n",
        "        return model\n",
        "\n",
        "\n",
        "    def unet(self,inputs, nb_classes=4, start_ch=64, depth=3, inc_rate=2. ,activation='relu', dropout=0.0, batchnorm=True, upconv=True,format_='channels_last'):\n",
        "        \"\"\"\n",
        "        the actual u-net architecture\n",
        "        \"\"\"\n",
        "        o = self.level_block(inputs,start_ch, depth, inc_rate,activation, dropout, batchnorm, upconv,format_)\n",
        "        o = BatchNormalization()(o) \n",
        "        #o =  Activation('relu')(o)\n",
        "        o=PReLU(shared_axes=[1, 2])(o)\n",
        "        o = Conv2D(nb_classes, 1, padding='same',data_format = format_)(o)\n",
        "        o = Activation('softmax')(o)\n",
        "        return o\n",
        "\n",
        "\n",
        "\n",
        "    def level_block(self,m, dim, depth, inc, acti, do, bn, up,format_=\"channels_last\"):\n",
        "        if depth > 0:\n",
        "            n = self.res_block_enc(m,0.0,dim,acti, bn,format_)\n",
        "            #using strided 2D conv for donwsampling\n",
        "            m = Conv2D(int(inc*dim), 2,strides=2, padding='same',data_format = format_)(n)\n",
        "            m = self.level_block(m,int(inc*dim), depth-1, inc, acti, do, bn, up )\n",
        "            if up:\n",
        "                m = UpSampling2D(size=(2, 2),data_format = format_)(m)\n",
        "                m = Conv2D(dim, 2, padding='same',data_format = format_)(m)\n",
        "            else:\n",
        "                m = Conv2DTranspose(dim, 3, strides=2,padding='same',data_format = format_)(m)\n",
        "            n=concatenate([n,m])\n",
        "            #the decoding path\n",
        "            m = self.res_block_dec(n, 0.0,dim, acti, bn, format_)\n",
        "        else:\n",
        "            m = self.res_block_enc(m, 0.0,dim, acti, bn, format_)\n",
        "        \n",
        "        return m\n",
        "\n",
        "  \n",
        "   \n",
        "    def res_block_enc(self,m, drpout,dim,acti, bn,format_=\"channels_last\"):\n",
        "        \n",
        "        \"\"\"\n",
        "        the encoding unit which a residual block\n",
        "        \"\"\"\n",
        "        n = BatchNormalization()(m) if bn else n\n",
        "        #n=  Activation(acti)(n)\n",
        "        n=PReLU(shared_axes=[1, 2])(n)\n",
        "        n = Conv2D(dim, 3, padding='same',data_format = format_)(n)\n",
        "                \n",
        "        n = BatchNormalization()(n) if bn else n\n",
        "        #n=  Activation(acti)(n)\n",
        "        n=PReLU(shared_axes=[1, 2])(n)\n",
        "        n = Conv2D(dim, 3, padding='same',data_format =format_ )(n)\n",
        "\n",
        "        n=add([m,n]) \n",
        "        \n",
        "        return  n \n",
        "\n",
        "\n",
        "\n",
        "    def res_block_dec(self,m, drpout,dim,acti, bn,format_=\"channels_last\"):\n",
        "\n",
        "        \"\"\"\n",
        "        the decoding unit which a residual block\n",
        "        \"\"\"\n",
        "         \n",
        "        n = BatchNormalization()(m) if bn else n\n",
        "        #n=  Activation(acti)(n)\n",
        "        n=PReLU(shared_axes=[1, 2])(n)\n",
        "        n = Conv2D(dim, 3, padding='same',data_format = format_)(n)\n",
        "\n",
        "        n = BatchNormalization()(n) if bn else n\n",
        "        #n=  Activation(acti)(n)\n",
        "        n=PReLU(shared_axes=[1, 2])(n)\n",
        "        n = Conv2D(dim, 3, padding='same',data_format =format_ )(n)\n",
        "        \n",
        "        Save = Conv2D(dim, 1, padding='same',data_format = format_,use_bias=False)(m) \n",
        "        n=add([Save,n]) \n",
        "        \n",
        "        return  n  \n",
        "    print(\"\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTyeQnPjybmV",
        "colab_type": "text"
      },
      "source": [
        "# Losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kh_iUUzeybmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dice(y_true, y_pred):\n",
        "    #computes the dice score on two tensors\n",
        "\n",
        "    sum_p=K.sum(y_pred,axis=0)\n",
        "    sum_r=K.sum(y_true,axis=0)\n",
        "    sum_pr=K.sum(y_true * y_pred,axis=0)\n",
        "    dice_numerator =2*sum_pr\n",
        "    dice_denominator =sum_r+sum_p\n",
        "    dice_score =(dice_numerator+K.epsilon() )/(dice_denominator+K.epsilon())\n",
        "    \n",
        "    return dice_score\n",
        "  \n",
        "def dice_core_metric(y_true, y_pred):\n",
        "    ##computes the dice for the core region\n",
        "\n",
        "    y_true_f = K.reshape(y_true,shape=(-1,4))\n",
        "    y_pred_f = K.reshape(y_pred,shape=(-1,4))\n",
        "    \n",
        "    #workaround for tf\n",
        "    #y_core=K.sum(tf.gather(y_true_f, [1,3],axis =1),axis=1)\n",
        "    #p_core=K.sum(tf.gather(y_pred_f, [1,3],axis =1),axis=1)\n",
        "    \n",
        "    y_core_1 = y_true_f[:,1]\n",
        "    y_core_3 = y_true_f[:,3]\n",
        "    y_core = y_core_1 + y_core_3 \n",
        "    p_core_1 = y_pred_f[:,1]\n",
        "    p_core_3 = y_pred_f[:,3]\n",
        "    p_core = p_core_1 + p_core_3 \n",
        "    dice_core=dice(y_core,p_core)\n",
        "    return dice_core\n",
        "\n",
        "\n",
        "def dice_whole_metric(y_true, y_pred):\n",
        "    #computes the dice for the whole tumor\n",
        "\n",
        "    y_true_f = K.reshape(y_true,shape=(-1,4))\n",
        "    y_pred_f = K.reshape(y_pred,shape=(-1,4))\n",
        "    y_whole=K.sum(y_true_f[:,1:],axis=1)\n",
        "    p_whole=K.sum(y_pred_f[:,1:],axis=1)\n",
        "    dice_whole=dice(y_whole,p_whole)\n",
        "    \n",
        "    return dice_whole\n",
        "\n",
        "def dice_en_metric(y_true, y_pred):\n",
        "    #computes the dice for the enhancing region\n",
        "\n",
        "    y_true_f = K.reshape(y_true,shape=(-1,4))\n",
        "    y_pred_f = K.reshape(y_pred,shape=(-1,4))\n",
        "    y_enh=y_true_f[:,-1]\n",
        "    p_enh=y_pred_f[:,-1]\n",
        "    dice_en=dice(y_enh,p_enh)\n",
        "    \n",
        "    return dice_en\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def weighted_log_loss(y_true, y_pred):\n",
        "    # scale predictions so that the class probas of each sample sum to 1\n",
        "    y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
        "    # clip to prevent NaN's and Inf's\n",
        "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
        "    # weights are assigned in this order : normal,necrotic,edema,enhancing \n",
        "    weights=np.array([1,5,2,4])\n",
        "    weights = K.variable(weights)\n",
        "    loss = y_true * K.log(y_pred) * weights\n",
        "    loss = K.mean(-K.sum(loss, -1))\n",
        "    \n",
        "    return loss\n",
        "\n",
        "def gen_dice_loss(y_true, y_pred):\n",
        "    '''\n",
        "    computes the sum of two losses : generalised dice loss and weighted cross entropy\n",
        "    '''\n",
        "\n",
        "    #generalised dice score is calculated as in this paper : https://arxiv.org/pdf/1707.03237\n",
        "    y_true_f = K.reshape(y_true,shape=(-1,4))\n",
        "    y_pred_f = K.reshape(y_pred,shape=(-1,4))\n",
        "    sum_p=K.sum(y_pred_f,axis=-2)\n",
        "    sum_r=K.sum(y_true_f,axis=-2)\n",
        "    sum_pr=K.sum(y_true_f * y_pred_f,axis=-2)\n",
        "    weights=K.pow(K.square(sum_r)+K.epsilon(),-1)\n",
        "    generalised_dice_numerator =2*K.sum(weights*sum_pr)\n",
        "    generalised_dice_denominator =K.sum(weights*(sum_r+sum_p))\n",
        "    generalised_dice_score =generalised_dice_numerator /generalised_dice_denominator\n",
        "    GDL=1-generalised_dice_score\n",
        "    del sum_p,sum_r,sum_pr,weights\n",
        "    print(GDL+weighted_log_loss(y_true,y_pred))\n",
        "    return GDL+weighted_log_loss(y_true,y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VJO1Szn6x2E",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oST-Z4ZRybmZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SGDLearningRateTracker(Callback):\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        optimizer = self.model.optimizer\n",
        "        lr = K.get_value(optimizer.lr)\n",
        "        decay = K.get_value(optimizer.decay)\n",
        "        lr=lr/10\n",
        "        decay=decay*10\n",
        "        K.set_value(optimizer.lr, lr)\n",
        "        K.set_value(optimizer.decay, decay)\n",
        "        print('LR changed to:',lr)\n",
        "        print('Decay changed to:',decay)\n",
        "\n",
        "\n",
        "\n",
        "class Training(object):\n",
        "    \n",
        "\n",
        "    def __init__(self, batch_size,nb_epoch,load_model_resume_training=None):\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.nb_epoch = nb_epoch\n",
        "\n",
        "        #loading model from path to resume previous training without recompiling the whole model\n",
        "        if load_model_resume_training is not None:\n",
        "            self.model =load_model(load_model_resume_training,custom_objects={'gen_dice_loss': gen_dice_loss,'dice_whole_metric':dice_whole_metric,'dice_core_metric':dice_core_metric,'dice_en_metric':dice_en_metric})\n",
        "            print(\"pre-trained model loaded!\")\n",
        "        else:\n",
        "            unet =Unet_model(img_shape=(128,128,4))\n",
        "            self.model=unet.model\n",
        "            print(\"U-net CNN compiled!\")\n",
        "\n",
        "                    \n",
        "    def fit_unet(self,X33_train,Y_train,X_patches_valid=None,Y_labels_valid=None):\n",
        "\n",
        "        train_generator=self.img_msk_gen(X33_train,Y_train,9999)\n",
        "        checkpointer = ModelCheckpoint(filepath='drive/My Drive/Brain Tumor Segmentation/ResUnet.{epoch:02d}_{val_loss:.3f}.hdf5', verbose=1)\n",
        "        self.model.fit_generator(train_generator,steps_per_epoch=len(X33_train)//self.batch_size,epochs=self.nb_epoch, validation_data=(X_patches_valid,Y_labels_valid),verbose=1, callbacks = [checkpointer,SGDLearningRateTracker()])\n",
        "        #self.model.fit(X33_train,Y_train, epochs=self.nb_epoch,batch_size=self.batch_size,validation_data=(X_patches_valid,Y_labels_valid),verbose=1, callbacks = [checkpointer,SGDLearningRateTracker()])\n",
        "\n",
        "    def img_msk_gen(self,X33_train,Y_train,seed):\n",
        "\n",
        "        '''\n",
        "        a custom generator that performs data augmentation on both patches and their corresponding targets (masks)\n",
        "        '''\n",
        "        datagen = ImageDataGenerator(horizontal_flip=True,data_format=\"channels_last\")\n",
        "        datagen_msk = ImageDataGenerator(horizontal_flip=True,data_format=\"channels_last\")\n",
        "        image_generator = datagen.flow(X33_train,batch_size=4,seed=seed)\n",
        "        y_generator = datagen_msk.flow(Y_train,batch_size=4,seed=seed)\n",
        "        while True:\n",
        "            yield(image_generator.next(), y_generator.next())\n",
        "\n",
        "    def save_model(model_name):\n",
        "        '''\n",
        "        INPUT string 'model_name': path where to save model and weights, without extension\n",
        "        Saves current model as json and weights as h5df file\n",
        "        '''\n",
        "        model_tosave = '{}.json'.format(model_name)\n",
        "        weights = '{}.hdf5'.format(model_name)\n",
        "        json_string = self.to_json()\n",
        "        self.model.save_weights(weights)\n",
        "        with open(model_tosave, 'w') as f:\n",
        "            json.dump(json_string, f)\n",
        "        print ('Model saved.')\n",
        "  \n",
        "\n",
        "    def load_model(self, model_name):\n",
        "        '''\n",
        "        Load a model\n",
        "        INPUT  (1) string 'model_name': filepath to model and weights, not including extension\n",
        "        OUTPUT: Model with loaded weights. can fit on model using loaded_model=True in fit_model method\n",
        "        '''\n",
        "        print ('Loading model {}'.format(model_name))\n",
        "        model_toload = '{}.json'.format(model_name)\n",
        "        weights = '{}.hdf5'.format(model_name)\n",
        "        with open(model_toload) as f:\n",
        "            m = next(f)\n",
        "        model_comp = model_from_json(json.loads(m))\n",
        "        model_comp.load_weights(weights)\n",
        "        print ('Model loaded.')\n",
        "        self.model = model_comp\n",
        "        return model_comp\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9fdwbpdybmc",
        "colab_type": "code",
        "outputId": "879d43b1-71f7-4ddc-f7bc-122c09f726c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    #set arguments\n",
        "\n",
        "    #reload already trained model to resume training\n",
        "    model_to_load=\"drive/My Drive/Brain Tumor Segmentation/ResUnet.01_0.728.hdf5\" \n",
        "    #save=None\n",
        "\n",
        "    #compile the model\n",
        "    brain_seg = Training(batch_size=4,nb_epoch=1,load_model_resume_training=model_to_load)\n",
        "\n",
        "    print(\"number of trainabale parameters:\",brain_seg.model.count_params())\n",
        "    #print(brain_seg.model.summary())\n",
        "    #plot(brain_seg.model, to_file='model_architecture.png', show_shapes=True)\n",
        "\n",
        "    #load data from disk\n",
        "    \n",
        "    \n",
        "    print(\"loading patches done\\n\")\n",
        "\n",
        "    # fit model\n",
        "    brain_seg.fit_unet(X_patches,Y_labels,X_patches_valid,Y_labels_valid)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Tensor(\"loss/activation_1_loss/add_2:0\", shape=(), dtype=float32)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "pre-trained model loaded!\n",
            "number of trainabale parameters: 10159748\n",
            "loading patches done\n",
            "\n",
            "Epoch 1/1\n",
            "LR changed to: 7.999999215826392e-05\n",
            "Decay changed to: 0.004999999655410647\n",
            "   1/1752 [..............................] - ETA: 7:23:37 - loss: 1.0215 - dice_whole_metric: 0.7900 - dice_en_metric: 3.2471e-09 - dice_core_metric: 1.2534e-04"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biY6RG4Tybmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGlVuCu8ybmq",
        "colab_type": "code",
        "outputId": "ea0ab3ca-6467-4bfb-f955-f8ef52e48a42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7QMnO4Dzt1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y = np.load(\"drive/My Drive/Brain Tumor Segmentation/Training Array/Y_labels_128.npy\").astype(np.uint8) #Don't use /content while loading a file\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPgNDd9DLYPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.load(\"drive/My Drive/Brain Tumor Segmentation/Training Array/X_patches_128.npy\").astype(np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iIyYfUf1jDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_patches,X_patches_valid,Y_labels,Y_labels_valid = train_test_split(X, Y, test_size=0.20, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLgFxzcu78xC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUyxRDb5F4JT",
        "colab_type": "code",
        "outputId": "179d0b9e-1288-4f60-96f5-4bdbe6f1b43f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8760, 128, 128, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JtAIva2F7ui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}